---
layout: post
title:      "The CLI Data Gem: I've Come This Far"
date:       2019-10-26 02:27:47 +0000
permalink:  the_cli_data_gem_ive_come_this_far
---


It feels like ages since I've sat at my computer desk doing a Learn.co assigned lesson. Completing a lab was like a small victory every time, but the projects are a different story. I remember opening the project page for the very first time. I took my time to read all the details, reading over the checklist multiple times and watching Avi's video on making his Daily Deals CLI Gem. It was all so intimidating. This was no lab. There was no setup this time. It all came down to me using what I've learned and being able to research some things I still had yet to learn. The first project was to be built from the ground up, and it was to use everything I've been taught about Procedural and Object Oriented Ruby. 

Looking at the project page for the first time was like a giant slap to the face. The Daily Deals video was a huge wakeup call for my growing ego. The sight of that checklist made my stomach churn. The example provided about the Gem for the World's Best Restaurants seemed to tower over me and what I've learned so far. How was I supposed to do this on my own? I've only had labs to do this and they always had tests for me to see if I was heading in the right direction. My cohort lead explained to me that I would still be testing my code, just in a different way. Instead of using spec tests, I would actually test this code by running the program and seeing if it responds the way I would like it to. It's more difficult to test code this way, but ultimately, I found it much more enjoyable.

I rewatched the Daily Deals video multiple times, clinging to the notepad Avi whipped up in the very beginning. He made notes to always write the code he wished he had, and then taking it step by step to make it all work. I decided I would try it his way. After all, I hadn't really come up with my own way to approach this monster of an assignment anyway, so thanks to his advice, I started with the environment. I added gemspec dependencies, set up which files to load in certain order, and made simple method in the main bin file to call to make sure my code was running. This alone took me a couple of days to really get it right, as my namespace variable was causing the whole program to break. After meeting with my cohort lead, we decided to ditch the namespace for the time being and to just get the code to function without so I could get to doing the actual mechanics of the project. I guess Highest Grossing Films probably wasn't the best name choice.

I built the basic flow for how I wanted my CLI to behave. That didn't take very long, as Avi taught me to use dummy data just to get it to work in the beginning, then replace it with real data when it comes time to do so. Following his advice, it was time to move on to the real data. I was going to try to scrape a periodical in order to display the top 20 best selling films of all time. There have been scraping labs and study groups to prepare me for this, and my cohort lead Juan had showed us a way to to narrow down a path to the info we wanted. Simple, right? No, wrong. Always wrong. It's never as simple as it seems, but that's okay. Trial and error goes a long way.

After trying extensively to scrape the movie data from the article, I had extensive difficulties separating the data into something readable. I had all the info at my fingertips and in my program, but I just couldn't seem to get it separated how I wanted it, so I began my search for different websites to use. I came across an IMDB page about the top grossing films of all time, and it was a large table that spanned to up to 700 films. The first page had the first 100. *This is great!*  I thought, *I can use this table instead to pull even more objects and more info, and it would be dynamic enough to for my application to change along with the website's info!* I thought about aiming for the first 50. It couldn't be that bad, right? I was terribly mistaken. Scraping the table would have been fairly simple, just a matter of sorting through all the nested cells and pulling their values. However, the base values on the main page were lacking. It was only the title of the film, that film's rank, and total sales. I wanted more, so I tried to scrape the main page and then somehow use that information to scrape each individual film's profile page. Of course, knowing this, I would stop it at 20 instead of trying to do 100.

Soon after this decision, I met with Juan for the regularly scheduled cohort study group. I presented both options to him, and he did warn me that scraping each and every individual page would be a painstaking process that would most likely slow my program down substantially. It was definitely accomplishable, but for the sake of my sanity, I was prepared to turn back to the article. Soon after that, he mentioned trying to teach all of us how to use an API as opposed to scraping. I'd hoped using an API would be less frustrating since he always spoke so highly of it.

He ran the study group and showed us examples. I followed along but it all seemed rather complex and I was having trouble following it. After the study group was over, he suggested a different website for me to try that would give me the same information I was looking for, only it had an easily accessible API to use instead of trying to scrape. With his guidance, I fledged out an API class, downloaded a JSON formatter extension for my Chrome browser, and got to work pulling info for my Gem. 

A recurring issue found its way into this process. I could make an API call on a list of movies, but the information about those movies was scarce. It didn't have most of the information readily available that I wanted. I met with Juan again and we looked over the API docs, trying to find a way to manipulate the return data from the API call. After a few minutes of searching, he had another meeting to attend to, so I continued to search solo for a little while longer. I had my mind set on using this API, I wasn't about to let it go. I stumbled across making a custom list of movies, that when the list was called on, it would present the information I was looking for. Halfway through my list, I received a message on Slack asking how the search was going.

After his other meeting, he jumped back in to look it over with me again. We were now having trouble with the list, but we stumbled across a specific movie call. This API code took a movie's ID number and plugged it into the website, displaying extensive information about that film. This was all extensive information I really wanted to include in the Gem. After brainstorming some, he suggested the idea of making two different API calls. Make an initial call based on the year, then make individual calls through interpolating the movie ID's the was pulled from the initial call. Ultimately, this suggestion became the basis for my entire Gem.

I took that advice and rolled with it. Now, instead of having a program that displayed *only* the highest grossing films of all time, I could now make a program that would allow the user to enter a year and see the highest grossing films of that specific year, still with the option to enter 'all' and see the list for all-time. Juan was excited for it and it made me excited, too. I finally had something to work with, and I took that and ran.

After the classes were built for making the API calls and generating them into movies that would be saved, it was time to replace the dummy data in my CLI class with real data. This part of the project took me very little time. I had already prepared what I wanted my CLI workflow to look like, so it was simply a matter of working in the real data that would be pulled from the website. After that was done, I sat in my chair in awe. I had gone from having a program that would show the user a list of the best selling films and then some details to allowing the user to specify a year, pulling the movies from that year, and then updating and displaying additional information. The Gem is very simple compared to just about any other real web application, but it's my first true creation at Flatiron School, and I couldn't be more proud of it. Thank you to all of my cohort mates that are always ready and willing to help whenever possible, and thank you to Juan for all your help with everything. Thanks to them, this project went from being something that towered over me to something I was able to be a part of and gain a deeper understanding about.

This blog post and CLI project were both made with <3, by Cody Dupuis, a student at Flatiron School.
